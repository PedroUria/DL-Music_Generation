\documentclass[man, floatsintext, 10pt]{apa6}
% floatsintext so that figures show up where we want them to (https://www.tug.org/pracjourn/2012-1/beitzel/beitzel.pdf)
\usepackage{lipsum}
\usepackage{amsmath}


\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage[american]{babel}
\usepackage{ulem}
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}

\title{Some cool title}

\shorttitle{Some cool title}

\author{Aaron A. Gauthier, Avijeet Kartikay and Pedro Uria Rodriguez \\ Machine Learning II}

\affiliation{The George Washington University}

\begin{document}
\maketitle

\section{Introduction}

\paragraph{Background} In the 6th century AD, Pythagoras proposed a concept called “The Music of the Spheres” to describe the proportional movements of the celestial bodies in the sky. However, this was not “music” as we think of it today, but rather a mathematical representation. Math and Music are intrinsically intertwined. The field of algorithmic composition dates back to the early days of Computer Science. Hiller and Isaacson at the University of Illinois Urbana-Champaign were the first to use computers to create music. They programmed the university’s Illiac I computer system to generate music tones based on random numbers. They utilized a model based on Markov Chains to predict the future path (probabilities) of a music note. If the note was not a fit, a new note was generated. The score that was generated in 1957 was called the Iliac suite string quartet. However, the music community did not immediately accept this as a true form of music, even though the scientific journals accepted and published their work. It was not until after Hillers death that their work was accepted and published within the music community. Today generative programs such as iTunes amongst others are commonplace to aid humans in composing music.  
There are 3 different types of models used for the composition of music. The first is a translation model. Translation models are rule-based models that take a non-musical medium like a picture and translate it into a rule-based algorithm for music. An example would be if it sees a horizontal line in a picture the rule may translate it as a constant pitch, whereas if it sees a vertical line it may translate it as a moving pitch or extended scale. The second is based on genetic algorithms such as mutation and natural selection. Different compositions essentially evolve into a suitable composition. The third model is the learning model, which is what we will focus on. In particular, we plan to use Deep Neural Networks to learn the underlying probability distribution of the music piece and generate new music based on this. Our first attempt will be using RNNs, given the temporal nature of music, but MLPs and CNNs will also be considered.

\paragraph{Data} We will be using Musical Instrument Digital Interface (MIDI) files. MIDI is basically a musical alphabet. You can think of it as a sequence of events, where for each event there is information about the note/rest/chord being played and its duration, together with the tempo, the intensity and other musical metadata. It supports multiple tracks and instruments. There are some sources listed on the referenced paper where we can get the data from. We will start with piano music, and then potentially move on to other types of instruments. There might be a challenge on the data size (too small), because we assume the model would get confused if we feed it very different types of music, so we will actually train a model for each genre and perhaps even author and style. A technique called transposition might be a solution to this problem. Another problem is that the quality of the MIDI files on the internet varies a lot, so we will have to make sure we are getting good data. The MIDI files will be encoded in order to feed them to our ANNs.

\paragraph{Networks} There are many different approaches to this problem. We will start small and train a RNN (possibly LSTM) on only the melody (right hand/voice). Then we will attempt to combine both melody and harmony (right/left hands, or even more than two voices). If this does not work (or even if it does), there is also the possibility of using a MLP/CNN to generate harmony for our generated melody. We will probably attempt, or perhaps even need, to customize the networks in some aspects, which we will realize once we get into the architecture design phase.

\paragraph{Frameworks} To preprocess the MIDI files, we will use Python’s music21 library together with NumPy. This will consist of encoding our data in order to input it to our ANNs, and then decoding the ANNs’ output to be able to get back the MIDI file and listen to the generated music. In the Deep Learning aspect, we will consider all three frameworks we have seen (or will see) during the course (PyTorch, Caffe and TensorFlow), and pick the one that best suits our needs. 

\paragraph{Evaluation} To evaluate our results, the most obvious way is listening to the generated music. We might need to come up with technical metrics if our results are bad, and may do so even if they are good, to compare our outputs and inputs. 


\section{Data}

Mm

\section{Encoding}

Mmm

\section{Network Architecture}

\paragraph{Overview} As we have mentioned, we used an LSTM to generate music. This was also the reason we encoded the music as a sequence of notes, including the hold dimension. The network consists of an LTSM with an arbitrary number of neurons as the Hidden layer, followed by a fully connected layer with the same number of neurons as input dimensions as the Output Layer. We need this exact number of neurons because we want to predict the next note after a sequence of input notes, thus the output dimensions must be equal to the input dimensions. These will be 89 when generating melodies, and 178 when generating melodies and harmonies at the same time. 

\vspace{2mm}

\paragraph{Output Layer} Because we are using a multi-many-hot encoding approach, we actually have a \textit{multi-label} classification problem. That is, each output vector can belong to more than one class, as we have the hold dimension and also more than one note can be played at the same time. Then, we cannot use the regular \textit{softmax} transfer function on the Output Layer, because this would only classify the output as a single note, as it models the joint probability of each dimension of the output multi-many-hot encoded vector. Indeed, this would result in only one dimension having a probability close to 1, while the rest are close to 0. Instead, we use a sigmoid function, which will model the probability of each dimension of our output vector individually, thus allowing for multiple probabilities close to 1, indicating that multiple notes and/or notes of larger duration are being generated.

\vspace{2mm}

\paragraph{Performance Index} The loss function used is then also different than \textit{Categorical Cross-Entropy}. We are using \textit{Binary Cross-Entropy}, because we are treating each dimension individually, so we only have two classes for each of them (0 or 1) but our output vector can have many class labels at the same time (many 1s). 

\section{Training}

\paragraph{On a single song} After designing the architecture, we had to think about the training process. First, we wrote our code to train the network on only one note, in order to evaluate its ability to learn the pattern within the sequence of notes in such song. To do so, we choose a sequence of notes to input our network of arbitrary length, and then we compute the loss between the predicted output and the real one, that is, the next sequence of notes. So if we input the sequence $[0, 1, 2, 3]$, where each integer represents a note and its temporal position, then we compute the loss between the output and $[1, 2, 3, 4]$. We also wrote another approach where the loss is only computed between $[4]$ and the last note of the output sequence. After computing this loss, we go backwards TODO: SPECIFY METHOD and update the weights and biases. Then we do the same thing with $[1, 2, 3, 4]$ as input and $[2, 3, 4, 5]$ as target, inputing also the hidden and cell states from the last iteration to the LSTM. We repeat this process over many epochs. We also added a condition on the code that decreased the learning rate by half when the loss does not decrease for 10 epochs, as we found this to be a somewhat common occurrence. 

\vspace{2mm}

\paragraph{On many songs} After making sure our network was able to reproduce a single song with a high degree of accuracy, which was done by using a generator function that will be explained later, we expanded our code to work on many songs. Our first approach was .... TODO .... 

Our definitive approach consists on the following: We simply stack all the encoded notes of all the songs on the training data together, sequentially. We end up with a huge matrix of sequences of notes, which does not distinguish where one song ends and where another song begins. Then we just input this matrix in the same way we did on our approach for only one song. Because looping over the whole songs takes a long time when we use a lot of them, we add the option of looping over only the first $n$ sequences of each song. This way the network will not see the full songs, but will still learn some patterns, possibly more than enough, given that usually, the songs contain repeating patterns along their body of notes. Because we are also using many songs, our network will still see many different patterns. 

\section{Generator}

Once we have trained our LSTM on some data, the final step is to use this network to generate new music. In order to do so, we used two slightly different approaches. The first one consists on ....

The second approach only takes a single note $[1]$ as initial input. Then, we go forward and get a new note $[2]$ as output. This note is actually a vector of probabilities, so we convert it to our multi-many-hot encoded vector of 1s and 0s by selecting either all the dimensions with probability greater than an arbitrary threshold, or if all the dimensions have probabilities less than this threshold, we select the dimension with the highest probability. In the case of generating melody + harmony, we do this separately for each hand. We also add another threshold to include the hold when the probability of such hold is close to the highest probability. After deciding on the encoded note, we add this note to the sequence of notes we will be using as input, so now we have $[1, 2]$, and go forward again to generate, in this case, a sequence of two notes $[2', 3]$. This time we only take the last generated note $[3]$ and add it to the next input sequence $[1, 2, 3]$. We do this again and again, until we reach a sequence length equal to the one our network had been trained with. When we reach this length, say we have $[1, 2, 3, ..., i]$, we use this sequence as input, but when we get our next generated note $[j]$, we remove the first note from our input sequence and add this new latest note, so on the next iteration we will input $[2, 3, ...., i, j]$. We repeat this until the number of wanted generated steps has been reached. Note that each output note (last note of each output sequence) is being added to a list of generated notes even when being removed from our input sequence at some point, and that we can keep going on forever and generate songs of any duration. We also remove the first generated notes until the constant sequence length has been reached, because these notes will not be representative of a network which was trained with a larger length of sequences.

TODO: TALK ABOUT GETTING STUCK ON A PARTICULAR NOTE AND A PARTICULAR SEQUENCE, ALTHOUGH THE CONDITIONS TO AVOID THIS ARE DEFAULT TO FALSE. ALSO ABOUT THE REMOVING RESTS STUFF.

\section{Experiments and Results}

\paragraph{Hyperparameters}

Uhuh

\paragraph{Best Results} 

Blabla

\section{Conclusions}

Mmmm









\newpage

%\begin{thebibliography}{9}
%\bibitem{latexcompanion} 
%Sebastian Raschka and Vahid Mirjalili. 
%\textit{Python Machine Learning}. 
%Packt, Birmingham, 2017.
%\https://arxiv.org/pdf/1709.01620.pdf
%\http://hagan.okstate.edu/NNDesign.pdf
 
%\end{thebibliography}

\end{document}
