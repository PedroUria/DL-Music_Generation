File number 0 loaded
File number 1 loaded
There was an error encoding this file /home/ubuntu/DL-Music_Generation/data/classical/haydn/sonata/haydn_7_2.mid
File number 3 loaded
File number 4 loaded
File number 5 loaded
File number 6 loaded
File number 7 loaded
There was an error encoding this file /home/ubuntu/DL-Music_Generation/data/classical/haydn/sonata/haydn_8_2.mid
File number 9 loaded
The loading process took 6 seconds
['/home/ubuntu/DL-Music_Generation/data/classical/haydn/sonata/haydn_33_2.mid', '/home/ubuntu/DL-Music_Generation/data/classical/haydn/sonata/haydn_9_2.mid', '/home/ubuntu/DL-Music_Generation/data/classical/haydn/sonata/haydn_7_2.mid', '/home/ubuntu/DL-Music_Generation/data/classical/haydn/sonata/haydn_8_3.mid', '/home/ubuntu/DL-Music_Generation/data/classical/haydn/sonata/haydn_35_1.mid', '/home/ubuntu/DL-Music_Generation/data/classical/haydn/sonata/haydn_43_1.mid', '/home/ubuntu/DL-Music_Generation/data/classical/haydn/sonata/haydn_35_3.mid', '/home/ubuntu/DL-Music_Generation/data/classical/haydn/sonata/hay_40_2.mid', '/home/ubuntu/DL-Music_Generation/data/classical/haydn/sonata/haydn_8_2.mid', '/home/ubuntu/DL-Music_Generation/data/classical/haydn/sonata/haydn_43_3.mid']
---------- epoch number: 0 ----------
           loss: 0.02396958
---------- epoch number: 1 ----------
           loss: 0.0077677337
---------- epoch number: 2 ----------
           loss: 0.005213224
---------- epoch number: 3 ----------
           loss: 0.003912724
---------- epoch number: 4 ----------
           loss: 0.0028162438
---------- epoch number: 5 ----------
           loss: 0.0021773456
---------- epoch number: 6 ----------
           loss: 0.0018401559
---------- epoch number: 7 ----------
           loss: 0.0015773755
---------- epoch number: 8 ----------
           loss: 0.0014242877
---------- epoch number: 9 ----------
           loss: 0.0013238253
---------- epoch number: 10 ----------
           loss: 0.0011775519
---------- epoch number: 11 ----------
           loss: 0.0010659132
---------- epoch number: 12 ----------
           loss: 0.0009781643
---------- epoch number: 13 ----------
           loss: 0.00093827286
---------- epoch number: 14 ----------
           loss: 0.00094476633
---------- epoch number: 15 ----------
           loss: 0.000948858
---------- epoch number: 16 ----------
           loss: 0.0008955668
---------- epoch number: 17 ----------
           loss: 0.0008603566
---------- epoch number: 18 ----------
           loss: 0.0008348558
---------- epoch number: 19 ----------
           loss: 0.00082738575
---------- epoch number: 20 ----------
           loss: 0.00082428614
---------- epoch number: 21 ----------
           loss: 0.0007649939
---------- epoch number: 22 ----------
           loss: 0.00075653876
---------- epoch number: 23 ----------
           loss: 0.0007343411
---------- epoch number: 24 ----------
           loss: 0.00074736617
---------- epoch number: 25 ----------
           loss: 0.00071963307
---------- epoch number: 26 ----------
           loss: 0.00074035896
---------- epoch number: 27 ----------
           loss: 0.0007577992
---------- epoch number: 28 ----------
           loss: 0.00075751904
---------- epoch number: 29 ----------
           loss: 0.00074601965
---------- epoch number: 30 ----------
           loss: 0.00069665303
---------- epoch number: 31 ----------
           loss: 0.0006913217
---------- epoch number: 32 ----------
           loss: 0.0006444324
---------- epoch number: 33 ----------
           loss: 0.0006850454
---------- epoch number: 34 ----------
           loss: 0.0006376118
---------- epoch number: 35 ----------
           loss: 0.0006483893
---------- epoch number: 36 ----------
           loss: 0.0006487569
---------- epoch number: 37 ----------
           loss: 0.000622408
---------- epoch number: 38 ----------
           loss: 0.0007377005
Halving learning rate from 0.01 to 0.005
---------- epoch number: 39 ----------
           loss: 0.0005748009
---------- epoch number: 40 ----------
           loss: 0.00045552125
---------- epoch number: 41 ----------
           loss: 0.00047413268
---------- epoch number: 42 ----------
           loss: 0.00044445434
---------- epoch number: 43 ----------
           loss: 0.00044237488
---------- epoch number: 44 ----------
           loss: 0.00045412913
---------- epoch number: 45 ----------
           loss: 0.0004319343
---------- epoch number: 46 ----------
           loss: 0.0004078906
---------- epoch number: 47 ----------
           loss: 0.00038572983
---------- epoch number: 48 ----------
           loss: 0.0003989335
---------- epoch number: 49 ----------
           loss: 0.00037890617
---------- epoch number: 50 ----------
           loss: 0.0004029341
---------- epoch number: 51 ----------
           loss: 0.0004050647
---------- epoch number: 52 ----------
           loss: 0.0003954075
---------- epoch number: 53 ----------
           loss: 0.00035734352
---------- epoch number: 54 ----------
           loss: 0.0003649082
---------- epoch number: 55 ----------
           loss: 0.00037410625
---------- epoch number: 56 ----------
           loss: 0.00034264024
---------- epoch number: 57 ----------
           loss: 0.00036156183
---------- epoch number: 58 ----------
           loss: 0.00034487504
---------- epoch number: 59 ----------
           loss: 0.00037827186
Halving learning rate from 0.005 to 0.0025
---------- epoch number: 60 ----------
           loss: 0.00040080582
---------- epoch number: 61 ----------
           loss: 0.00031809154
---------- epoch number: 62 ----------
           loss: 0.00033296176
---------- epoch number: 63 ----------
           loss: 0.00030527796
---------- epoch number: 64 ----------
           loss: 0.00031747276
---------- epoch number: 65 ----------
           loss: 0.00030625417
---------- epoch number: 66 ----------
           loss: 0.00031016892
---------- epoch number: 67 ----------
           loss: 0.00029894063
---------- epoch number: 68 ----------
           loss: 0.00029657202
---------- epoch number: 69 ----------
           loss: 0.00029375812
---------- epoch number: 70 ----------
           loss: 0.0002963815
---------- epoch number: 71 ----------
           loss: 0.00029016766
---------- epoch number: 72 ----------
           loss: 0.00034594562
Halving learning rate from 0.0025 to 0.00125
---------- epoch number: 73 ----------
           loss: 0.0003674063
---------- epoch number: 74 ----------
           loss: 0.00031237878
---------- epoch number: 75 ----------
           loss: 0.00031345902
---------- epoch number: 76 ----------
           loss: 0.00029433038
---------- epoch number: 77 ----------
           loss: 0.0002797936
---------- epoch number: 78 ----------
           loss: 0.0002793683
---------- epoch number: 79 ----------
           loss: 0.0003057087
---------- epoch number: 80 ----------
           loss: 0.00028574053
---------- epoch number: 81 ----------
           loss: 0.0003052942
---------- epoch number: 82 ----------
           loss: 0.00029238567
---------- epoch number: 83 ----------
           loss: 0.00027370773
---------- epoch number: 84 ----------
           loss: 0.00031933805
Halving learning rate from 0.00125 to 0.000625
---------- epoch number: 85 ----------
           loss: 0.00031698667
---------- epoch number: 86 ----------
           loss: 0.0002682935
---------- epoch number: 87 ----------
           loss: 0.00026200875
---------- epoch number: 88 ----------
           loss: 0.0002579872
---------- epoch number: 89 ----------
           loss: 0.00025762388
---------- epoch number: 90 ----------
           loss: 0.00025489402
---------- epoch number: 91 ----------
           loss: 0.00025123733
---------- epoch number: 92 ----------
           loss: 0.00023850103
---------- epoch number: 93 ----------
           loss: 0.00022977057
---------- epoch number: 94 ----------
           loss: 0.00022668521
---------- epoch number: 95 ----------
           loss: 0.00021599334
---------- epoch number: 96 ----------
           loss: 0.00022648301
---------- epoch number: 97 ----------
           loss: 0.0002126749
---------- epoch number: 98 ----------
           loss: 0.00019571524
---------- epoch number: 99 ----------
           loss: 0.00020175402

The training process took 1732.72 seconds

Printing out the maximum prob of all notes for a time step when this maximum prob is less than 0.9
66 left tensor([9.9163e-08], device='cuda:0')
