\documentclass[pdf]{beamer}

\usepackage{harmony}  % Music stuff % https://martin-thoma.com/how-to-write-music-with-latex/
\usepackage{hyperref}  % For links stuff
%\usepackage{listings}
\usepackage{amsmath} % Some math stuff
\usepackage{dirtytalk} % Quote
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}

\mode<presentation>{\usetheme{Warsaw}\usecolortheme{orchid}}
\beamertemplatenavigationsymbolsempty

\title[Classical 846 LSTM]{Classical 846 LSTM}

\author[Aaron A. Gauthier, Avijeet Kartikay and Pedro Uria Rodriguez]{Aaron A. Gauthier,  Avijeet Kartikay and Pedro Uria Rodriguez \\ \vspace{2mm}
Machine Learning II \\ \vspace{2mm}
 GWU}

\date{April 24, 2019}

\begin{document}

\maketitle

\begin{frame}{Table of Contents}
\tableofcontents[part=1]
\end{frame}

\begin{frame}{Table of Contents}
\tableofcontents[part=2]
\end{frame}


% -------------------------------------------------------------------------------- %

\part{1}

\section{Introduction}

\subsection{Objective}

\begin{frame}{Music Generation System}

\begin{itemize}
\item Build a Neural Network that can generate new music  \\
\pause
 \vspace{2mm}
\hspace{-6mm} Because of the temporal nature of music...
 \pause
 \vspace{2mm}
 \item We will use LSTM
 \pause
 \item We will train on many songs from an author and/or genre
 \pause
 \item The ANN will learn the probability distribution of a sequence of notes, i.e, the music
 \pause
 \item Using this distribution, we can predict the next note based on an arbitrary number of previous notes
 \end{itemize}

  
\end{frame}

%-----------------------------------------------------------%

\section{Data}

\subsection{Data Format}

\begin{frame}{MIDI}

\begin{itemize}
\item Protocol that stores music data and metadata, and allows different instruments and software to communicate with each other. \\
\vspace{5mm}
\pause
\hspace{-6mm} It is made up by a series of events, with info regarding:
\pause
\item Location in time
\item Duration in time
\item Pitch, intensity and tempo
\item Other metadata
\end{itemize}


\end{frame}

\subsection{Dataset/s}

\begin{frame}{Data Sources}

\begin{itemize}
\item Classical Piano Midi Page: \url{http://piano-midi.de/}
\item Folders Organized by Author and Genre
\item We want the data to be well organized 
\item We also have other sources but... \\
\pause
\end{itemize}

\say{How do I go about finding a certain classical work in MIDI format in the Internet? Some MIDI archives in the Internet contain thousands of classical works. You can find the corresponding links to them on my Linkpage. \textbf{The quality of the pieces, however, may vary considerably}} \\
\vspace{2mm}
From the author of \url{http://piano-midi.de/}. 
\end{frame}

%-----------------------------------------------------------%

\section{Encoding}

\subsection{General Thoughts}

\begin{frame}{Overview}

\begin{itemize}
\item MIDI files provide us with more info than we need
\pause
\item We are going to use a many-hot encoding approach \\
\pause
\vspace{2mm}
Thus...
\pause
\item Tempo will not be encoded: Too many possible values
\pause
\item Intensity will not be encoded: Same reason \\
\end{itemize}
\pause
We are essentially losing expressiveness info in order to reduce the complexity of the network.

\end{frame}

\subsection{Notes}

\begin{frame}{Many-One-Hot Encoding}
\begin{itemize}
\item Python's \texttt{music21} library to read \texttt{.mid} files.
\pause
\item Preprocess the \texttt{stream} objects to get the data for the time events.
\end{itemize}
\pause
\begin{figure}[ht]
   \begin{center}
       \includegraphics[width=
               \textwidth]{keyboard.jpg}
  \end{center}
  \end{figure}

\hspace{15mm} $ \bar{p}_t = [0, 0, ....., 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, ..., 0]  $
\pause
\begin{itemize}
\item Create a sequence where each input vector $\bar{p}_t$ corresponds to the duration of the shortest note on the piece/s: \\

\textit{time step} $ \equiv \Delta_t = t_1 - t_0 = t_2 - t_1 = ... = cte $
\end{itemize}

\end{frame}

\subsection{Rests and Holds}

\begin{frame}{Two extra dimensions}
\begin{itemize}
\item Rests are essential to music. Add component \#88 to encode rests.
\pause
\item If a note is longer than the \textit{time step}, it will be split into more than one vector. Suppose we have:
\end{itemize}
\pause
\textit{time step}  $=$  \Vier, event at $t_i =$ \Halb \hspace{2mm} \pause $\Rightarrow \bar{p}_{t_i} = [0, 0, ...., 1, 0, 0, ....]$ \pause and $\bar{p}_{t_{i+1}} = [0, 0, ...., 1, 0, 0, ....] = \bar{p}_{t_i}$ \pause $\Rightarrow$  \Vier \hspace{0.5mm}  \Vier \hspace{1mm} $\neq$ \Halb \hspace{2mm} \\

\pause
\begin{itemize}
\item Add \textit{hold} component \#89, which indicates that the notes played at a time event shall be held. We end up with:
\end{itemize}
\pause 
$ \bar{p}_{t_i} = [0, 0, ...., 1, 0, 0, ...., 1],  \bar{p}_{t_{i+1}} = [0, 0, ...., 1, 0, 0, ...., 0] \Rightarrow$ \Halb. 

\end{frame}

\subsection{Multivoice polyphony}

\begin{frame}{Combining Melody \& Harmony}

\begin{itemize}
\item Approach 1: Keep the same number of dimensions by just adding up the vectors from the left and right hands. \pause
\[\bar{p}_{t_i} = \overbrace{[0, 1, ...., 0, 0, 1]}^{\text{left hand}} + \overbrace{[0, ...., 0, 1, 0, 0]}^{\text{right hand}} = [0, 1, ....., 0, 1, 0, 1 ] \]
\pause \vspace{-6mm}
\item Problem: \textit{hold} $=1$ or \textit{hold} $=0$? \pause $\Rightarrow$ Duration of note for each hand is lost.
\pause
\item Approach 2: Stack the left and right vectors together horizontally, effectively doubling the number of dimensions.
\pause
 \[\bar{p}_{t_i} = \overbrace{[0, 1, ...., 0, 0, 1 |}^{\text{left hand}} \overbrace{0, ...., 0, 1, 0, 0,]}^{\text{right hand}} \] 

\end{itemize}

\end{frame}

%-----------------------------------------------------------%


\part{2}

\section{Neural Network stuff}

\begin{frame}{hhh}

TODO

\end{frame}


%-----------------------------------------------------------%

\section{Experimental Results}

\subsection{Different training approaches}
\begin{frame}{}
\begin{itemize}
\item Approach \#1 did not work. The network got confused and did not learn any patterns
\vspace{3mm}
\item Approach \#2 worked decently, but tweaks to the generator were needed.
\vspace{3mm}
\item  Approach \#3 worked well for some data, although it also has its flaws.
\end{itemize}
\end{frame}


\subsection{Hyperparameters}
\begin{frame}{Training Hyperparameters}
\begin{itemize}
\item \texttt{time\_step}: duration of each vector
\item \texttt{seq\_len}: number of time steps to input as a sequence
\item \texttt{hidden\_size}: number of neurons in LSTM layer/s
\item \texttt{lr}: learning rate
\item \texttt{n\_epoch}: number of training iterations
\item \texttt{use\_all\_seq}: uses all sequences of the pieces
\item \texttt{use\_n\_seq}: used when all sequence is false
\end{itemize}
\end{frame}

\subsection{Hyperparameters Adjusted}
\begin{frame}{Best Values in General}
\begin{itemize}
\item \texttt{time\_step}: 0.25
\item \texttt{seq\_len}: 30-50
\item\texttt{hidden\_size}: 178 for Melody $\&$ Harmony, 89 for Melody.
\item \texttt{lr}: 0.01
\item \texttt{n\_epoch}: 50 - 100
\item \texttt{use\_all\_seq}: \texttt{False}
\item \texttt{use\_n\_seq}: 100
\end{itemize}
\end{frame}

\subsection{Trained on various authors, individually}
\begin{frame}{Results}
Albeniz:
\begin{itemize}
\item Model did not predict well with this music - seems rather unpredictable
\item Tuned multiple hyperparameters such as \texttt{hidden\_size}, \texttt{time\_step}, \texttt{lr}, \texttt{n\_epochs}
\end{itemize}
Tschaikovsky:
\begin{itemize}
\item Model predicted well with this music - higher quality result
\item Tuned multiple hyperparameters such as \texttt{hidden\_size}, \texttt{time\_step}, \texttt{lr}, \texttt{n\_epochs}
\end{itemize}
Other authors:
\begin{itemize}
\item We tried many other authors, with various degrees of success
\item Also needed to tune hyperparameters for most of them 
\end{itemize}
\end{frame}

\subsection{Future Work}
\begin{frame}{So many things we could do}
\begin{itemize}
\item Use a mix of different authors.
\item We actually tried this but with a inherently flawed approach due to a lack of time
\item We could also work on different genres of music
\item Also different instruments, and potentially add more than two voices
\item We could even generate only melodies via LSTM and then map these melodies with harmonies training a MLP/CNN on real melodies as inputs and their harmonies as outputs.
\item Expand the network architecture by combining with other types
\end{itemize}
\end{frame}

%-----------------------------------------------------------------------------%
\section{Conclusion}

\subsection{Some last words}
\begin{frame}{Final Thoughts}
\begin{itemize}
\item Music generation is very complex!
\item Mathematics and music are intricately intertwined.
\item Multiple complexities and levels of difficulty associated with music: Time, Duration, Tempo
\item  Music Files: Quality of music files, Encoding, etc.
\item   Quality / Complexity of Model: Different models will yield different results
\item   Accuracy of Tools: PyTorch, Keras, etc.
\item   Infinite number of possible variables to tweak
\item Improvements for the Future:
\item  Generate harmonies for LSTM-generated melodies using MLP/CNN
\item  New models for other genres of music
\item  Improving the quality of output of the model
\end{itemize}

\end{frame}

\subsection{The End}

\begin{frame}{Happy Music Generation!}

\begin{figure}[ht]
   \begin{center}
       \includegraphics[width=0.7
               \textwidth]{piano-art.jpg}
  \end{center}
  \end{figure}
  \end{frame}
  
\begin{frame}{Sources}

\begin{itemize}
\item \url{https://arxiv.org/abs/1709.01620}
\item \url{http://hagan.okstate.edu/NNDesign.pdf}
\item MLII GWU class slides $\&$ notes.
\item \url{https://web.mit.edu/music21/doc/}
\item \url{https://pytorch.org/docs/stable/index.html}
\item \url{https://gombru.github.io/2018/05/23/cross_entropy_loss/}
\item \url{https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/}
\item \href{http://piano-midi.de/}{Classical Piano Midi Page}
\end{itemize}

You can play with our system at: \url{https://github.com/PedroUria/DL-Music_Generation}
  \end{frame}

\end{document}