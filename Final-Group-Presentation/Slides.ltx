\documentclass[pdf]{beamer}

\usepackage{harmony}  % Music stuff % https://martin-thoma.com/how-to-write-music-with-latex/
\usepackage{hyperref}  % For links stuff
%\usepackage{listings}
\usepackage{amsmath} % Some math stuff
\usepackage{dirtytalk} % Quote
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}

\mode<presentation>{\usetheme{Warsaw}\usecolortheme{orchid}}
\beamertemplatenavigationsymbolsempty

\title[Same title or shortened]{Some cool title}

\author[Aaron A. Gauthier, Avijeet Kartikay and Pedro Uria Rodriguez]{Aaron A. Gauthier,  Avijeet Kartikay and Pedro Uria Rodriguez \\ \vspace{2mm}
Machine Learning II \\ \vspace{2mm}
 GWU}

\date{April 24, 2019}

\begin{document}

\maketitle

\begin{frame}{Table of Contents}
\tableofcontents[part=1]
\end{frame}

\begin{frame}{Table of Contents}
\tableofcontents[part=2]
\end{frame}


% -------------------------------------------------------------------------------- %

\part{1}

\section{Introduction}

\subsection{Objective}

\begin{frame}{Music Generation System}

\begin{itemize}
\item Build a Neural Network that can generate new music  \\
\pause
 \vspace{2mm}
\hspace{-6mm} Because of the temporal nature of music...
 \pause
 \vspace{2mm}
 \item We will use LSTM
 \pause
 \item We will train on many songs from an author and/or genre
 \pause
 \item The ANN will learn the probability distribution of a sequence of notes, i.e, the music
 \pause
 \item Using this distribution, we can predict the next note based on an arbitrary number of previous notes
 \end{itemize}

  
\end{frame}

%-----------------------------------------------------------%

\section{Data}

\subsection{Data Format}

\begin{frame}{MIDI}

\begin{itemize}
\item Protocol that stores music data and metadata, and allows different instruments and software to communicate with each other. \\
\vspace{5mm}
\pause
\hspace{-6mm} It is made up by a series of events, with info regarding:
\pause
\item Location in time
\item Duration in time
\item Pitch, intensity and tempo
\item Other metadata
\end{itemize}


\end{frame}

\subsection{Dataset/s}

\begin{frame}{Data Sources}

\begin{itemize}
\item Classical Piano Midi Page: \url{http://piano-midi.de/}
\item Folders Organized by Author and Genre
\item We want the data to be well organized 
\item We also have other sources but... \\
\end{itemize}

\say{How do I go about finding a certain classical work in MIDI format in the Internet? Some MIDI archives in the Internet contain thousands of classical works. You can find the corresponding links to them on my Linkpage. \textbf{The quality of the pieces, however, may vary considerably}} \\
\vspace{2mm}
From the author of \url{http://piano-midi.de/}. 
\end{frame}

%-----------------------------------------------------------%

\section{Encoding}

\subsection{General Thoughts}

\begin{frame}{Overview}

\begin{itemize}
\item MIDI files provide us with more info than we need
\pause
\item We are going to use a many-hot encoding approach \\
\pause
\vspace{2mm}
Thus...
\pause
\item Tempo will not be encoded: Too many possible values
\item Intensity will not be encoded: Same reason \\
\end{itemize}
We are essentially losing expressiveness info in order to reduce the complexity of the network.

\end{frame}

\subsection{Notes}

\begin{frame}{Many-one-hot Encoding}
\begin{itemize}
\item Python's \texttt{music21} library to read \texttt{.mid} files.
\item Preprocess the \texttt{stream} objects to get the data for the time events.
\end{itemize}
\pause
\begin{figure}[ht]
   \begin{center}
       \includegraphics[width=
               \textwidth]{keyboard.jpg}
  \end{center}
  \end{figure}

\hspace{15mm} $ \bar{p}_t = [0, 0, ....., 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, ..., 0]  $
\pause
\begin{itemize}
\item Create a sequence where each input vector $\bar{p}_t$ corresponds to the duration of the shortest note on the piece/s: \\

\textit{time step} $ \equiv \Delta_t = t_1 - t_0 = t_2 - t_1 = ... = cte $
\end{itemize}

\end{frame}

\subsection{Rests and Holds}

\begin{frame}{Two extra dimensions}
\begin{itemize}
\item Rests are essential to music. Add component \#88 to encode rests.
\pause
\item If a note is longer than the \textit{time step}, it will be split into more than one vector. Suppose we have:
\end{itemize}
\pause
\textit{time step}  $=$  \Vier, event at $t_i =$ \Halb \hspace{2mm} \pause $\Rightarrow \bar{p}_{t_i} = [0, 0, ...., 1, 0, 0, ....]$ \pause and $\bar{p}_{t_{i+1}} = [0, 0, ...., 1, 0, 0, ....] = \bar{p}_{t_i}$ \pause $\Rightarrow$  \Vier \hspace{0.5mm}  \Vier \hspace{1mm} $\neq$ \Halb \hspace{2mm} \\

\pause
\begin{itemize}
\item Add \textit{hold} component \#89, which indicates that the notes played at a time event shall be held. We end up with:
\end{itemize}
\pause 
$ \bar{p}_{t_i} = [0, 0, ...., 1, 0, 0, ...., 1],  \bar{p}_{t_{i+1}} = [0, 0, ...., 1, 0, 0, ...., 0] \Rightarrow$ \Halb. 

\end{frame}

\subsection{Multivoice polyphony}

\begin{frame}{Combining Melody \& Harmony}

\begin{itemize}
\item Approach 1: Keep the same number of dimensions by just adding up the vectors from the left and right hands. \pause
\[\bar{p}_{t_i} = \overbrace{[0, 1, ...., 0, 0, 1]}^{\text{left hand}} + \overbrace{[0, ...., 0, 1, 0, 0]}^{\text{right hand}} = [0, 1, ....., 0, 1, 0, 1 ] \]
\pause \vspace{-6mm}
\item Problem: \textit{hold} $=1$ or \textit{hold} $=0$? \pause $\Rightarrow$ Duration of note for each hand is lost.
\pause
\item Approach 2: Stack the left and right vectors together horizontally, effectively doubling the number of dimensions.
\pause
 \[\bar{p}_{t_i} = \overbrace{[0, 1, ...., 0, 0, 1 |}^{\text{left hand}} \overbrace{0, ...., 0, 1, 0, 0,]}^{\text{right hand}} \] 

\end{itemize}

\end{frame}

%-----------------------------------------------------------%


\part{2}

\section{Neural Network stuff}

\begin{frame}{hhh}

TODO

\end{frame}


%-----------------------------------------------------------%

\section{Experimental Results}

\begin{frame}
\subsection{Hyperparameters}
\begin{itemize}
\ time_step: duration of each vector
\ seq_len: number of time steps to input as a sequence
\ hidden_size: number of neurons in LSTM layer
\ lr: learning rate
\ n_epoch: number of training iterations
\ use_all_seq: uses all sequences of the pieces
\ use_n_seq: used when all sequence is false
\end{itemize}
\subsection{Primary Hyperparameters adjusted:
\begin{itemize}
\ hidden_size
\ time_step
\ lr
\ n_epoch
\end{itemize}

\end{frame}

%----------------------------------------------------------%

\section{Classical Music Experimental Results}

\begin{frame}
\subsection{Albeniz}
\begin{itemize}
\ Model did not predict well with this music - seems rather “predictable”
\ Tuned multiple hyperparameters such as hidden_size, time_step, lr, n_epochs
\subsection{Tschaikovsky}
\ Model predicted well with this music - higher quality result
\ Tuned multiple hyperparameters such as hidden_size, time_step, lr, n_epochs
\subsection{Classical Mix}
\ Experimented with mixing Albeniz, Mendelsohn, Bach and Tschaivkovsky mix
\ Tuned multiple hyperparameters such as hidden_size, time_step, lr, n_epochs
\ Results were bad - possible “confusion” of the model 
\subsection{Mendelssohn}
\ Model predicted “decently” - higher quality result
\ Tuned multiple hyperparameters such as hidden_size, time_step, lr, n_epochs
\end{itemize}

%-----------------------------------------------------------------------------%
\section{Conclusion}

\begin{frame}
\
\begin{itemize}
\ Music generation is very complex!
\ Mathematics and music are intricately intertwined.
\ Multiple complexities and levels of difficulty associated with music:
\   Music: Time, Duration, Tempo
\   Music Files: Quality of music files, Encoding, etc.
\   Quality / Complesity of Model: Different models will yield different results
\   Accuracy of Tools: PyTorch, Keras, etc.
\   Infinite number of possible variables to tweak: Starting on a different note, Step, etc.
\ Improvements for the Future:
\  Generate melodies using MLP/CNN
\  New models for other genres of music
\  Improving the quality of output of the model
\end{itemize}

\end{frame}

%-----------------------------------------------------------------------------%

\begin{frame}{Whatever}

Something 

\end{frame}

\subsection{The End}

\begin{frame}{Happy Music Generation!}

Some cool pic!

\end{frame}

\end{document}



%\subsection{Domain Knowledge}

%\begin{frame}{Mushroom Features}
%\begin{columns}
%\begin{column}{0.5\textwidth}
%\begin{figure}[ht]
 %   \begin{center}
 %       \includegraphics[width=0.7\textwidth]{1797-09-03_Agaricus_campestris_Plate_by_James_Sowerby.jpg}
 %   \end{center}
   % \end{figure}
   % \end{column}
%\begin{column}{0.5\textwidth}
%\begin{figure}[ht]
  %  \begin{center}
    %    \includegraphics[width=1\textwidth]{k208z0y.png}
  %  \end{center}
   % \end{figure}
%Go to the \href{https://github.com/QuirkyDataScientist1978/GWU-Machine-Learning-1-Fall-2018-Mushroom-Classification-Project/blob/master/mushrooms.ipynb}{jupyter notebook} for an explanation of each of them. 
%\end{column}
%\end{columns}
%\end{frame}



%-----------------------------------------------------------%






%\begin{frame}{References}
%\begin{thebibliography}{1}
%\bibitem{latexcompanion} 
%Sebastian Raschka and Vahid Mirjalili. 
%\textit{Python Machine Learning}. 
%Packt, Birmingham, 2017

 
%\vspace{5mm}
%Github Repository: \footnotesize\url{https://github.com/PedroUria/DL-Music_Generation}
%\end{frame}
